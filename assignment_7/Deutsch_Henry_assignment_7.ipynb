{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84GqNz8Ztw4G"
   },
   "source": [
    "# Artificial Intelligence\n",
    "# 464/664\n",
    "# Assignment #7\n",
    "\n",
    "## General Directions for this Assignment\n",
    "\n",
    "00. We're using a Jupyter Notebook environment (tutorial available here: https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html),\n",
    "01. Output format should be exactly as requested (it is your responsibility to make sure notebook looks as expected on Gradescope),\n",
    "02. Check submission deadline on Gradescope,\n",
    "03. Rename the file to Last_First_assignment_7,\n",
    "04. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
    "05. Do not submit any other files.\n",
    "\n",
    "## Before You Submit...\n",
    "\n",
    "1. Re-read the general instructions provided above, and\n",
    "2. Hit \"Kernel\"->\"Restart & Run All\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSN1KpKJtw4K"
   },
   "source": [
    "## Neural Networks: Architecture\n",
    "\n",
    "For this assignment we will explore Neural Networks; in particular, we are going to explore model complexity. We will use the same dataset from Assignment #6 to classify a mushroom as either edible ('e') or poisonous ('p'). You are free to use PyTorch, TensorFlow, scikit-learn -- to name a few resources. The goal is to explore different model complexities (architectures) before declaring a winner. Either start with a simple network and make it more complex; or start with a complex model and pare it down. Either way, your submission should clearly demonstrate your exploration.\n",
    "\n",
    "\n",
    "Your output for each model should look like the output of `cross_validate` from Assignment #6:\n",
    "\n",
    "```\n",
    "Fold: 0\tTrain Error: 15.38%\tValidation Error: 0.00%\n",
    "Fold: 1\n",
    "...\n",
    "\n",
    "Mean(Std. Dev.) over all folds:\n",
    "-------------------------------\n",
    "Train Error: 100.00%(0.00%) Test Error: 100.00%(0.00%)\n",
    "```\n",
    "\n",
    "Notice that \"Test Error\" has been replaced by \"Validation Error.\" Split your dataset into train, test, and validation sets.\n",
    "\n",
    "\n",
    "Start with a simple network. Train using the train set. Observe model's performance using the validation set.\n",
    "\n",
    "\n",
    "Increase the complexity of your network. Train using the train set. Observe model's performance using the validation set.\n",
    "\n",
    "\n",
    "Model complexity in Assignment #6 was depth limit. You can think of it here as the architecture of the network (number of layers and units per layer). Try at least three different network architectures.\n",
    "\n",
    "\n",
    "We're trying to find a model complexity that generalizes well. (Recall high bias vs high variance discussion in class.)\n",
    "\n",
    "\n",
    "Pick the network architecture that you deem best. Use the test set to report your winning model's performance. This is the ONLY time you use the test set.\n",
    "\n",
    "\n",
    "Try at least three different models; more importantly, document your process: what the results were, how the winning model was determined, what was the winning model's performance on the test data. Clearly highlight these items to receive full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3-RPRoiXtw4L"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow info and warning messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process & Encode Mushroom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "            'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "            'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "            'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "            'stalk-color-below-ring', 'veil-type', 'veil-color',\n",
    "            'ring-number', 'ring-type', 'spore-print-color',\n",
    "            'population', 'habitat']\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('agaricus-lepiota.data', names=columns)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class'].apply(lambda x: 1 if x == 'e' else 0)  # Encode target: edible=1, poisonous=0\n",
    "\n",
    "# One-hot encode features\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)\n",
    "\n",
    "# Convert data to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement K-Fold Cross Validation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K folds: helper function for cross validation\n",
    "def create_folds(data: List, n: int) -> List[List[List]]:\n",
    "  k, m = divmod(len(data), n)\n",
    "  return list(data[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "# Do k-fold cross validation\n",
    "def cross_validate(get_model, X, y, n_folds=5):\n",
    "  Xy = list(zip(X.to_numpy(), y.to_numpy()))\n",
    "  folds = create_folds(Xy, n_folds)\n",
    "  train_errors, validation_errors = [], []\n",
    "  for i, fold in enumerate(folds):\n",
    "    # Convert validation data to numpy arrays\n",
    "    X_validate, y_validate = map(list, zip(*fold))  # Convert to lists first\n",
    "    X_validate = np.array(X_validate)\n",
    "    y_validate = np.array(y_validate)\n",
    "    \n",
    "    # Initialize training data\n",
    "    X_train_data = []\n",
    "    y_train_data = []\n",
    "    \n",
    "    # Collect training data from other folds\n",
    "    for j, fold2 in enumerate(folds):\n",
    "      if i == j: continue\n",
    "      X_fold, y_fold = map(list, zip(*fold2))  # Convert to lists first\n",
    "      X_train_data.extend(X_fold)\n",
    "      y_train_data.extend(y_fold)\n",
    "    \n",
    "    # Convert training data to numpy arrays\n",
    "    X_train = np.array(X_train_data)\n",
    "    y_train = np.array(y_train_data)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model = get_model()\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_validate, y_validate), verbose=0)\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    train_error_100 = (1 - train_acc[-1]) * 100\n",
    "    validation_error_100 = (1 - val_acc[-1]) * 100\n",
    "    train_errors.append(train_error_100)\n",
    "    validation_errors.append(validation_error_100)\n",
    "    print(f\"Fold {i}: Training Error: {train_error_100:.2g}%\\tValidation Error: {validation_error_100:.2g}%\")\n",
    "  print(f\"Mean(Std. Dev.) over all folds:\")\n",
    "  print(f\"-------------------------------\")\n",
    "  print(f\"Train Error: {np.mean(train_errors):.2f}%({np.std(train_errors):.2f}%)\\tValidation Error: {np.mean(validation_errors):.2g}%({np.std(validation_errors):.2g}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dense layer\n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super(MyDenseLayer, self).__init__()\n",
    "    self.W = self.add_weight(shape=(input_dim, output_dim), initializer='random_normal')\n",
    "    self.b = self.add_weight(shape=(output_dim,), initializer='zeros')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    z = tf.matmul(inputs, self.W) + self.b\n",
    "    output = tf.math.sigmoid(z)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Different Model Architectures For Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    MyDenseLayer(input_dim=X_train.shape[1], output_dim=4),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def medium_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    MyDenseLayer(input_dim=X_train.shape[1], output_dim=16),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def medium_large_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    MyDenseLayer(input_dim=X_train.shape[1], output_dim=16),\n",
    "    MyDenseLayer(input_dim=16, output_dim=16),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def large_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    MyDenseLayer(input_dim=X_train.shape[1], output_dim=32),\n",
    "    MyDenseLayer(input_dim=32, output_dim=32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def large_model_v2():\n",
    "  model = tf.keras.Sequential([\n",
    "    MyDenseLayer(input_dim=X_train.shape[1], output_dim=64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Training Error: 11%\tValidation Error: 12%\n",
      "Fold 1: Training Error: 8.8%\tValidation Error: 8.2%\n",
      "Fold 2: Training Error: 11%\tValidation Error: 9%\n",
      "Fold 3: Training Error: 11%\tValidation Error: 10%\n",
      "Fold 4: Training Error: 11%\tValidation Error: 12%\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 10.45%(0.87%)\tValidation Error: 10%(1.6%)\n"
     ]
    }
   ],
   "source": [
    "cross_validate(small_model, X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Medium Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Training Error: 9.7%\tValidation Error: 11%\n",
      "Fold 1: Training Error: 9.2%\tValidation Error: 8.4%\n",
      "Fold 2: Training Error: 11%\tValidation Error: 9.1%\n",
      "Fold 3: Training Error: 9.9%\tValidation Error: 8.4%\n",
      "Fold 4: Training Error: 9.2%\tValidation Error: 11%\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 9.77%(0.61%)\tValidation Error: 9.5%(1.1%)\n"
     ]
    }
   ],
   "source": [
    "cross_validate(medium_model, X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Medium-Large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Training Error: 48%\tValidation Error: 49%\n",
      "Fold 1: Training Error: 48%\tValidation Error: 49%\n",
      "Fold 2: Training Error: 48%\tValidation Error: 49%\n",
      "Fold 3: Training Error: 49%\tValidation Error: 46%\n",
      "Fold 4: Training Error: 48%\tValidation Error: 48%\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 48.24%(0.20%)\tValidation Error: 48%(1.1%)\n"
     ]
    }
   ],
   "source": [
    "cross_validate(medium_large_model, X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Training Error: 43%\tValidation Error: 49%\n",
      "Fold 1: Training Error: 48%\tValidation Error: 49%\n",
      "Fold 2: Training Error: 48%\tValidation Error: 49%\n",
      "Fold 3: Training Error: 44%\tValidation Error: 46%\n",
      "Fold 4: Training Error: 47%\tValidation Error: 48%\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 46.02%(1.98%)\tValidation Error: 48%(1.1%)\n"
     ]
    }
   ],
   "source": [
    "cross_validate(large_model, X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Large Model v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Training Error: 9.9%\tValidation Error: 11%\n",
      "Fold 1: Training Error: 9.4%\tValidation Error: 8.3%\n",
      "Fold 2: Training Error: 9.7%\tValidation Error: 8.5%\n",
      "Fold 3: Training Error: 10%\tValidation Error: 9.1%\n",
      "Fold 4: Training Error: 9.2%\tValidation Error: 11%\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 9.67%(0.33%)\tValidation Error: 9.5%(1.1%)\n"
     ]
    }
   ],
   "source": [
    "cross_validate(large_model_v2, X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on results\n",
    "\n",
    "Probably unsurprisingly, the smaller model with just 1 hidden layer with 4 nodes performs badly, and medium performs better. \n",
    "\n",
    "Something that I didn't expect is that my next attempt — adding more layers w/ the medium_large_model — actually did considerably worse. The new layer actually detracts from the models performance. I then tried to add more nodes per layer, along with the increased number of layers — the medium -> large model comparison. This also did considerably worse. It seems like either adding another layer or increasing the total number of nodes detracts from performance.\n",
    "\n",
    "To try to isolate the variable of what exactly was making the model much worse — increasing the total number of nodes, or increasing the number of layers — I created the large_model_v2, which kept the 1 layer architecture, but with 64 nodes total. This performed just about exactly as well as the medium model, but at a much bigger size. So this should indicate that it is primarily the increase in the numebr of layers that detracts from performance. I think this should mean that you can conceptualize the relative order of poision levels of mushrooms as a linear function of the features (I won't say that you can conceptualize the *absolute* poison level of a mushroom as a linear combo of features because of the softmax).\n",
    "\n",
    "If you trained the larger models for more time and with more data, they should theoretically perform no worse, beacuse the second layer can always just mimick the first with weights of 1 for the corresponding node on the same vertical level. However, given the practical considerations of this homework, you should expect that unnecessary complexity in architecture should make the models perform worse, beause all models are trained on the same amount of data.\n",
    "\n",
    "**Winner: medium model**. This model performs at least as well as everything else at a small size, at least for SGD optimization and sigmoid activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small sgd: Training Error: 11%\tValidation Error: 11%\n",
      "medium sgd: Training Error: 8.3%\tValidation Error: 7.1%\n",
      "medium-large sgd: Training Error: 48%\tValidation Error: 48%\n",
      "large sgd: Training Error: 39%\tValidation Error: 48%\n",
      "large single-hidden-layer sgd: Training Error: 8.2%\tValidation Error: 6.2%\n"
     ]
    }
   ],
   "source": [
    "# Small Model\n",
    "models = {\n",
    "  'small sgd': small_model(),\n",
    "  'medium sgd': medium_model(),\n",
    "  'medium-large sgd': medium_large_model(),\n",
    "  'large sgd': large_model(),\n",
    "  'large single-hidden-layer sgd': large_model_v2(),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "  history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=0)\n",
    "  train_acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  train_error_100 = (1 - train_acc[-1]) * 100\n",
    "  validation_error_100 = (1 - val_acc[-1]) * 100\n",
    "  print(f\"{name}: Training Error: {train_error_100:.2g}%\\tValidation Error: {validation_error_100:.2g}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeaSsuWCt2dI"
   },
   "source": [
    "## Experiment: Activation Function and Optimizer\n",
    "Modify the 1) Activation function 2) Optimizer of any chosen model. Try at least one model for each modified component.\n",
    "\n",
    "Explain the motivation behind the modifications you made.\n",
    "\n",
    "Explore the effects on the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation: Modifying Activation Function\n",
    "\n",
    "Hard sigmoid is a piecewise linear approximation of sigmoid. It's faster to compute than sigmoid, although we won't take advantage of that here since we're still only using one layer. However, it's also less prone to vanishing gradients, so it may improve the medium model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WaYqkeqQyanB"
   },
   "outputs": [],
   "source": [
    "# Implementation and exploration.\n",
    "def medium_model_hard_sigmoid_activation():\n",
    "  model = tf.keras.Sequential([\n",
    "    MyDenseLayer(input_dim=X_train.shape[1], output_dim=16),\n",
    "    tf.keras.layers.Dense(1, activation='hard_sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Training Error: 10%\tValidation Error: 11%\n",
      "Fold 1: Training Error: 11%\tValidation Error: 10%\n",
      "Fold 2: Training Error: 11%\tValidation Error: 9%\n",
      "Fold 3: Training Error: 11%\tValidation Error: 10%\n",
      "Fold 4: Training Error: 10%\tValidation Error: 11%\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 10.57%(0.43%)\tValidation Error: 10%(0.85%)\n"
     ]
    }
   ],
   "source": [
    "cross_validate(medium_model_hard_sigmoid_activation, X_train, y_train, n_folds=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Effects on performance\n",
    "\n",
    "The hard sigmoid activation seems to slightly detract from the performance of the medium model. Vanishing gradients are mostly a concern when you're dealing with very deep neural networks, so that factor may not have played a big role here. Sigmoid is smoother though, which is supposed to be better for optimization — so that may be the only relevant factor here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation: Modifying Optimizer\n",
    "\n",
    "Adam adapts the learning rate for each parameter individually — this is particularly helpful in deeper/larger networks where different parameters might need different scales of updates. So, I think it makes more sense to try Adam on the large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_model_adam_optimizer():\n",
    "  model = tf.keras.Sequential([\n",
    "    MyDenseLayer(input_dim=X_train.shape[1], output_dim=32),\n",
    "    MyDenseLayer(input_dim=32, output_dim=32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Training Error: 0%\tValidation Error: 0.18%\n",
      "Fold 1: Training Error: 0.044%\tValidation Error: 0%\n",
      "Fold 2: Training Error: 0.022%\tValidation Error: 0%\n",
      "Fold 3: Training Error: 0%\tValidation Error: 0%\n",
      "Fold 4: Training Error: 0%\tValidation Error: 0%\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.01%(0.02%)\tValidation Error: 0.035%(0.07%)\n"
     ]
    }
   ],
   "source": [
    "cross_validate(large_model_adam_optimizer, X_train, y_train, n_folds=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Effects on performance\n",
    "\n",
    "Adam seems to improve the performance of the large model a LOT. My best guess as to why would be that my hypothesis was mostly correct: Adam (the idea of having adaptive learning rates) is supposed to converge faster, but has higher memory requirements. However when comparing the large sgd vs large adam models, we don't really consider memory requirements or convergence speed, so adam just seems much better despite probably being slower and using more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJDsC809yqTw"
   },
   "source": [
    "## OPTIONAL. BONUS. Experiment: Loss Function\n",
    "\n",
    "Modify the loss function of any chosen model.\n",
    "\n",
    "Explain the motivation behind the modifications you made.\n",
    "\n",
    "Explore the effects on the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation: Modifying Loss Function\n",
    "\n",
    "The large model with 2 layers might produce more extreme predictions. MSE could do better for the large model (w/ adam optimizer and sigmoid activation) because it penalizes larger errors more heavily (quadratically) compared to binary cross entropy. Also, MSE's gradients are more stable for extreme predictions near 0 or 1, unlike BCE which can give very large gradients in those cases. So MSE may be worth trying over BCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9zavtQjCy7Ud"
   },
   "outputs": [],
   "source": [
    "# change loss function of large adam model w/ sigmoid\n",
    "def large_model_mse_loss():\n",
    "  model = tf.keras.Sequential([\n",
    "    MyDenseLayer(input_dim=X_train.shape[1], output_dim=32),\n",
    "    MyDenseLayer(input_dim=32, output_dim=32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Training Error: 0%\tValidation Error: 0.18%\n",
      "Fold 1: Training Error: 0%\tValidation Error: 0%\n",
      "Fold 2: Training Error: 0%\tValidation Error: 0%\n",
      "Fold 3: Training Error: 0%\tValidation Error: 0%\n",
      "Fold 4: Training Error: 0%\tValidation Error: 0%\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.00%(0.00%)\tValidation Error: 0.035%(0.07%)\n"
     ]
    }
   ],
   "source": [
    "cross_validate(large_model_mse_loss, X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation: Effects on performance\n",
    "\n",
    "Both binary cross entropy and mean squared errors do pretty well. MSE seems to do slightly worse, but I'm not sure if this is a statistically significant difference. BCE may actually be more appropriate because we're doing a binary classification task — BCE is specifically designed to model probability distributions for binary outcomes, which maps well to our edible/poisonous mushrooms case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMCU5PBHz8DF"
   },
   "source": [
    "No other directions for this assignment, other than what's here and in the \"General Directions\" section. You have a lot of freedom with this assignment. Don't get carried away. It is expected the results may vary, being better or worse, due to the limitations of the dataset. Graders are not going to run your notebooks. The notebook will be read as a report on how different models were explored. Since you'll be using libraries, the emphasis will be on your ability to communicate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VfoAYAQtw4M"
   },
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Re-read the general instructions provided above, and\n",
    "2. Hit \"Kernel\"->\"Restart & Run All\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
