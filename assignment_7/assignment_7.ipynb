{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84GqNz8Ztw4G"
   },
   "source": [
    "# Artificial Intelligence\n",
    "# 464/664\n",
    "# Assignment #7\n",
    "\n",
    "## General Directions for this Assignment\n",
    "\n",
    "00. We're using a Jupyter Notebook environment (tutorial available here: https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html),\n",
    "01. Output format should be exactly as requested (it is your responsibility to make sure notebook looks as expected on Gradescope),\n",
    "02. Check submission deadline on Gradescope,\n",
    "03. Rename the file to Last_First_assignment_7,\n",
    "04. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
    "05. Do not submit any other files.\n",
    "\n",
    "## Before You Submit...\n",
    "\n",
    "1. Re-read the general instructions provided above, and\n",
    "2. Hit \"Kernel\"->\"Restart & Run All\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSN1KpKJtw4K"
   },
   "source": [
    "## Neural Networks: Architecture\n",
    "\n",
    "For this assignment we will explore Neural Networks; in particular, we are going to explore model complexity. We will use the same dataset from Assignment #6 to classify a mushroom as either edible ('e') or poisonous ('p'). You are free to use PyTorch, TensorFlow, scikit-learn -- to name a few resources. The goal is to explore different model complexities (architectures) before declaring a winner. Either start with a simple network and make it more complex; or start with a complex model and pare it down. Either way, your submission should clearly demonstrate your exploration.\n",
    "\n",
    "\n",
    "Your output for each model should look like the output of `cross_validate` from Assignment #6:\n",
    "\n",
    "```\n",
    "Fold: 0\tTrain Error: 15.38%\tValidation Error: 0.00%\n",
    "Fold: 1\n",
    "...\n",
    "\n",
    "Mean(Std. Dev.) over all folds:\n",
    "-------------------------------\n",
    "Train Error: 100.00%(0.00%) Test Error: 100.00%(0.00%)\n",
    "```\n",
    "\n",
    "Notice that \"Test Error\" has been replaced by \"Validation Error.\" Split your dataset into train, test, and validation sets.\n",
    "\n",
    "\n",
    "Start with a simple network. Train using the train set. Observe model's performance using the validation set.\n",
    "\n",
    "\n",
    "Increase the complexity of your network. Train using the train set. Observe model's performance using the validation set.\n",
    "\n",
    "\n",
    "Model complexity in Assignment #6 was depth limit. You can think of it here as the architecture of the network (number of layers and units per layer). Try at least three different network architectures.\n",
    "\n",
    "\n",
    "We're trying to find a model complexity that generalizes well. (Recall high bias vs high variance discussion in class.)\n",
    "\n",
    "\n",
    "Pick the network architecture that you deem best. Use the test set to report your winning model's performance. This is the ONLY time you use the test set.\n",
    "\n",
    "\n",
    "Try at least three different models; more importantly, document your process: what the results were, how the winning model was determined, what was the winning model's performance on the test data. Clearly highlight these items to receive full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3-RPRoiXtw4L"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the mushroom dataset\n",
    "def load_and_preprocess_data(filepath='agaricus-lepiota.data'):\n",
    "  # Define column names\n",
    "  columns = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "              'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "              'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "              'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "              'stalk-color-below-ring', 'veil-type', 'veil-color',\n",
    "              'ring-number', 'ring-type', 'spore-print-color',\n",
    "              'population', 'habitat']\n",
    "  \n",
    "  # Load data\n",
    "  df = pd.read_csv(filepath, names=columns)\n",
    "  \n",
    "  # Separate features and target\n",
    "  X = df.drop('class', axis=1)\n",
    "  y = df['class'].apply(lambda x: 1 if x == 'e' else 0)  # Encode target: edible=1, poisonous=0\n",
    "  \n",
    "  # One-hot encode features\n",
    "  X_encoded = pd.get_dummies(X)\n",
    "  \n",
    "  return X_encoded, y\n",
    "\n",
    "def create_folds(data: List, n: int) -> List[List[List]]:\n",
    "  k, m = divmod(len(data), n)\n",
    "  return list(data[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "def split_data(X, y):\n",
    "  X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=13)\n",
    "  X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=13)\n",
    "  return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Define a custom dense layer\n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super(MyDenseLayer, self).__init__()\n",
    "    self.W = self.add_weight(shape=(input_dim, output_dim), initializer='random_normal')\n",
    "    self.b = self.add_weight(shape=(output_dim,), initializer='zeros')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    z = tf.matmul(inputs, self.W) + self.b\n",
    "    output = tf.math.sigmoid(z)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_and_preprocess_data()\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "\n",
    "# Convert data to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train <class 'pandas.core.frame.DataFrame'>       cap-shape_b  cap-shape_c  cap-shape_f  cap-shape_k  cap-shape_s  \\\n",
      "3223          0.0          0.0          1.0          0.0          0.0   \n",
      "5696          0.0          0.0          1.0          0.0          0.0   \n",
      "5568          0.0          0.0          0.0          0.0          0.0   \n",
      "6806          0.0          0.0          1.0          0.0          0.0   \n",
      "2024          0.0          0.0          0.0          0.0          0.0   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "2790          0.0          0.0          1.0          0.0          0.0   \n",
      "7696          0.0          0.0          0.0          1.0          0.0   \n",
      "74            1.0          0.0          0.0          0.0          0.0   \n",
      "6320          0.0          0.0          0.0          0.0          0.0   \n",
      "338           0.0          0.0          0.0          0.0          0.0   \n",
      "\n",
      "      cap-shape_x  cap-surface_f  cap-surface_g  cap-surface_s  cap-surface_y  \\\n",
      "3223          0.0            0.0            0.0            0.0            1.0   \n",
      "5696          0.0            0.0            0.0            0.0            1.0   \n",
      "5568          1.0            0.0            0.0            1.0            0.0   \n",
      "6806          0.0            0.0            0.0            0.0            1.0   \n",
      "2024          1.0            1.0            0.0            0.0            0.0   \n",
      "...           ...            ...            ...            ...            ...   \n",
      "2790          0.0            1.0            0.0            0.0            0.0   \n",
      "7696          0.0            0.0            0.0            0.0            1.0   \n",
      "74            0.0            0.0            0.0            1.0            0.0   \n",
      "6320          1.0            0.0            0.0            0.0            1.0   \n",
      "338           1.0            1.0            0.0            0.0            0.0   \n",
      "\n",
      "      ...  population_s  population_v  population_y  habitat_d  habitat_g  \\\n",
      "3223  ...           0.0           0.0           1.0        1.0        0.0   \n",
      "5696  ...           0.0           0.0           1.0        0.0        1.0   \n",
      "5568  ...           0.0           1.0           0.0        0.0        1.0   \n",
      "6806  ...           0.0           1.0           0.0        1.0        0.0   \n",
      "2024  ...           0.0           0.0           1.0        1.0        0.0   \n",
      "...   ...           ...           ...           ...        ...        ...   \n",
      "2790  ...           0.0           0.0           1.0        1.0        0.0   \n",
      "7696  ...           0.0           1.0           0.0        1.0        0.0   \n",
      "74    ...           1.0           0.0           0.0        0.0        1.0   \n",
      "6320  ...           0.0           1.0           0.0        0.0        0.0   \n",
      "338   ...           0.0           1.0           0.0        0.0        0.0   \n",
      "\n",
      "      habitat_l  habitat_m  habitat_p  habitat_u  habitat_w  \n",
      "3223        0.0        0.0        0.0        0.0        0.0  \n",
      "5696        0.0        0.0        0.0        0.0        0.0  \n",
      "5568        0.0        0.0        0.0        0.0        0.0  \n",
      "6806        0.0        0.0        0.0        0.0        0.0  \n",
      "2024        0.0        0.0        0.0        0.0        0.0  \n",
      "...         ...        ...        ...        ...        ...  \n",
      "2790        0.0        0.0        0.0        0.0        0.0  \n",
      "7696        0.0        0.0        0.0        0.0        0.0  \n",
      "74          0.0        0.0        0.0        0.0        0.0  \n",
      "6320        0.0        0.0        1.0        0.0        0.0  \n",
      "338         0.0        0.0        0.0        1.0        0.0  \n",
      "\n",
      "[5686 rows x 117 columns] y_train <class 'pandas.core.series.Series'> 3223    1\n",
      "5696    0\n",
      "5568    0\n",
      "6806    0\n",
      "2024    1\n",
      "       ..\n",
      "2790    1\n",
      "7696    0\n",
      "74      1\n",
      "6320    0\n",
      "338     1\n",
      "Name: class, Length: 5686, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdfaa64fc90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple neural network model\n",
    "model = tf.keras.Sequential([\n",
    "  MyDenseLayer(input_dim=X_train.shape[1], output_dim=16),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('X_train', type(X_train), X_train, 'y_train', type(y_train), y_train)\n",
    "\n",
    "# do k-fold cross validation\n",
    "def cross_validate(model, X, y, n_folds=5):\n",
    "  Xy = list(zip(X, y))\n",
    "  folds = create_folds(Xy, n_folds)\n",
    "  for i, fold in enumerate(folds):\n",
    "    # Convert validation data to numpy arrays\n",
    "    X_validate, y_validate = map(list, zip(*fold))  # Convert to lists first\n",
    "    X_validate = np.array(X_validate)\n",
    "    y_validate = np.array(y_validate)\n",
    "    \n",
    "    # Initialize training data\n",
    "    X_train_data = []\n",
    "    y_train_data = []\n",
    "    \n",
    "    # Collect training data from other folds\n",
    "    for j, fold2 in enumerate(folds):\n",
    "      if i == j: continue\n",
    "      X_fold, y_fold = map(list, zip(*fold2))  # Convert to lists first\n",
    "      X_train_data.extend(X_fold)\n",
    "      y_train_data.extend(y_fold)\n",
    "    \n",
    "    # Convert training data to numpy arrays\n",
    "    X_train = np.array(X_train_data)\n",
    "    y_train = np.array(y_train_data)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_validate, y_validate), verbose=0)\n",
    "    \n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    print(f\"Fold {i}: Training Accuracy: {train_acc[-1]:.4f} Validation Accuracy: {val_acc[-1]:.4f}\")\n",
    "\n",
    "# cross_validate(model, X_train, y_train, n_folds=5)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "# Evaluate on test set\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "# test_accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Medium Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "178/178 [==============================] - 1s 2ms/step - loss: 0.5046 - accuracy: 0.8377 - val_loss: 0.2172 - val_accuracy: 0.9409\n",
      "Epoch 2/10\n",
      "178/178 [==============================] - 0s 996us/step - loss: 0.1274 - accuracy: 0.9689 - val_loss: 0.0593 - val_accuracy: 0.9877\n",
      "Epoch 3/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9914 - val_loss: 0.0263 - val_accuracy: 0.9967\n",
      "Epoch 4/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9974 - val_loss: 0.0138 - val_accuracy: 0.9992\n",
      "Epoch 5/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0130 - accuracy: 0.9989 - val_loss: 0.0092 - val_accuracy: 0.9984\n",
      "Epoch 6/10\n",
      "178/178 [==============================] - 0s 996us/step - loss: 0.0083 - accuracy: 0.9996 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0056 - accuracy: 0.9996 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a simple neural network model\n",
    "model = tf.keras.Sequential([\n",
    "  MyDenseLayer(input_dim=X_train.shape[1], output_dim=32),\n",
    "  MyDenseLayer(input_dim=32, output_dim=32),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "178/178 [==============================] - 1s 2ms/step - loss: 0.4819 - accuracy: 0.7147 - val_loss: 0.0696 - val_accuracy: 0.9852\n",
      "Epoch 2/10\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.9896 - val_loss: 0.0128 - val_accuracy: 0.9984\n",
      "Epoch 3/10\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9979 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.0030 - val_accuracy: 0.9992\n",
      "Epoch 5/10\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.9863e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 9.4877e-04 - accuracy: 1.0000 - val_loss: 7.6327e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 7.3213e-04 - accuracy: 1.0000 - val_loss: 6.1030e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 5.8165e-04 - accuracy: 1.0000 - val_loss: 4.8998e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 4.7410e-04 - accuracy: 1.0000 - val_loss: 4.0406e-04 - val_accuracy: 1.0000\n",
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Build a simple neural network model\n",
    "model = tf.keras.Sequential([\n",
    "  MyDenseLayer(input_dim=X_train.shape[1], output_dim=64),\n",
    "  MyDenseLayer(input_dim=64, output_dim=64),\n",
    "  MyDenseLayer(input_dim=64, output_dim=64),\n",
    "  MyDenseLayer(input_dim=64, output_dim=64),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeaSsuWCt2dI"
   },
   "source": [
    "## Experiment: Activation Function and Optimizer\n",
    "Modify the 1) Activation function 2) Optimizer of any chosen model. Try at least one model for each modified component.\n",
    "\n",
    "Explain the motivation behind the modifications you made.\n",
    "\n",
    "Explore the effects on the performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WaYqkeqQyanB"
   },
   "outputs": [],
   "source": [
    "# Implementation and exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJDsC809yqTw"
   },
   "source": [
    "## OPTIONAL. BONUS. Experiment: Loss Function\n",
    "\n",
    "Modify the loss function of any chosen model.\n",
    "\n",
    "Explain the motivation behind the modifications you made.\n",
    "\n",
    "Explore the effects on the performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9zavtQjCy7Ud"
   },
   "outputs": [],
   "source": [
    "# Implementation and exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMCU5PBHz8DF"
   },
   "source": [
    "No other directions for this assignment, other than what's here and in the \"General Directions\" section. You have a lot of freedom with this assignment. Don't get carried away. It is expected the results may vary, being better or worse, due to the limitations of the dataset. Graders are not going to run your notebooks. The notebook will be read as a report on how different models were explored. Since you'll be using libraries, the emphasis will be on your ability to communicate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VfoAYAQtw4M"
   },
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Re-read the general instructions provided above, and\n",
    "2. Hit \"Kernel\"->\"Restart & Run All\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
